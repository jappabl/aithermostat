{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5999 sum -3299.7237286847085              \r"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gym_environment\n",
    "import const\n",
    "\n",
    "env = gym_environment.Environment()\n",
    "\t\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# from the pytorch page on dqn\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "class ReplayMemory:\n",
    "\tdef __init__(self, size):\n",
    "\t\tself.memory = deque([], maxlen=size)\n",
    "\tdef push(self, *args):\n",
    "\t\tself.memory.append(Transition(*args))\n",
    "\tdef sample(self, batch_size):\n",
    "\t\treturn random.sample(self.memory, batch_size)\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.memory)\n",
    "\t\n",
    "class DQN(nn.Module):\n",
    "\tdef __init__(self, observation_size, action_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# self.fc1 = nn.Linear(observation_size, 128)\n",
    "\t\t# self.fc2 = nn.Linear(128, 128)\n",
    "\t\t# self.fc3 = nn.Linear(128, action_size)\n",
    "\t\tself.fc1 = nn.Linear(observation_size, 16)\n",
    "\t\tself.fc2 = nn.Linear(16, 32)\n",
    "\t\tself.fc3 = nn.Linear(32, 64)\n",
    "\t\tself.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = F.relu(self.fc3(x))\n",
    "\t\treturn self.fc4(x)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state, _ = env.reset()\n",
    "observation_size = len(state)\n",
    "\n",
    "policy_net = DQN(observation_size, action_size).to(const.DEVICE)\n",
    "target_net = DQN(observation_size, action_size).to(const.DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "\tglobal steps_done\n",
    "\tsample = random.random()\n",
    "\teps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\tsteps_done += 1\n",
    "\tif sample > eps_threshold:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# t.max(1) will return the largest column value of each row.\n",
    "\t\t\t# second column on max result is index of where max element was\n",
    "\t\t\t# found, so we pick action with the larger expected reward.\n",
    "\t\t\treturn policy_net(state).max(1).indices.view(1, 1)\n",
    "\telse:\n",
    "\t\treturn torch.tensor([[env.action_space.sample()]], device=const.DEVICE, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\t\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\tbatch = Transition(*zip(*transitions))\n",
    "\t# print(batch)\n",
    "\n",
    "\tnon_final_mask = torch.tensor(list(map(lambda s: s is not None, batch.next_state)), device=const.DEVICE, dtype=torch.bool)\n",
    "\tnon_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "\tstate_batch = torch.cat(batch.state)\n",
    "\taction_batch = torch.cat(batch.action)\n",
    "\treward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\t# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "\t# columns of actions taken. These are the actions which would've been taken\n",
    "\t# for each batch state according to policy_net\n",
    "\tstate_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "\t# Compute V(s_{t+1}) for all next states.\n",
    "\t# Expected values of actions for non_final_next_states are computed based\n",
    "\t# on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "\t# This is merged based on the mask, such that we'll have either the expected\n",
    "\t# state value or 0 in case the state was final.\n",
    "\tnext_state_values = torch.zeros(BATCH_SIZE, device=const.DEVICE)\n",
    "\twith torch.no_grad():\n",
    "\t\tnext_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\t# Compute the expected Q values\n",
    "\texpected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "\t# Compute Huber loss\n",
    "\tcriterion = nn.SmoothL1Loss()\n",
    "\tloss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\t# Optimize the model\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\t# In-place gradient clipping\n",
    "\ttorch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\toptimizer.step()\n",
    "\n",
    "# term_cols = os.get_terminal_size().columns\n",
    "\n",
    "# if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "# \tnum_episodes = 1000\n",
    "# else:\n",
    "# \tnum_episodes = 1000\n",
    "\n",
    "num_episodes = 6000\n",
    "# num_episodes = int(sys.argv[-1])\n",
    "\n",
    "xvalues = np.arange(1441)\n",
    "temps = np.zeros(1441)\n",
    "target = np.zeros(1441)\n",
    "reward2 = np.zeros(1441)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\trewards = []\n",
    "\ttotal_reward = 0\n",
    "\t# Initialize the environment and get its state\n",
    "\n",
    "\tstate, info = env.reset(num_setpoints=random.randint(2, 7))\n",
    "\t\n",
    "\tstate = state.unsqueeze(0)\n",
    "\n",
    "\tfor t in count():\n",
    "\t\taction = select_action(state)\n",
    "\t\tobservation, reward, terminated = env.step(action.item())\n",
    "\n",
    "\t\t# temps[t] = env._cur_temp\n",
    "\t\t# target[t] = env._target\n",
    "\t\t# reward2[t] = reward\n",
    "\n",
    "\t\treward = torch.tensor([reward], device=const.DEVICE)\n",
    "\t\tdone = terminated \n",
    "\n",
    "\t\tif terminated:\n",
    "\t\t\tnext_state = None\n",
    "\t\telse:\n",
    "\t\t\tnext_state = observation.unsqueeze(0)\n",
    "\n",
    "\t\t# Store the transition in memory\n",
    "\t\tmemory.push(state, action, next_state, reward)\n",
    "\n",
    "\t\t# Move to the next state\n",
    "\t\tstate = next_state\n",
    "\n",
    "\t\t# Perform one step of the optimization (on the policy network)\n",
    "\t\toptimize_model()\n",
    "\n",
    "\t\t# Soft update of the target network's weights\n",
    "\t\t# θ′ ← τ θ + (1 −τ )θ′\n",
    "\t\ttarget_net_state_dict = target_net.state_dict()\n",
    "\t\tpolicy_net_state_dict = policy_net.state_dict()\n",
    "\t\tfor key in policy_net_state_dict:\n",
    "\t\t\ttarget_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\t\ttarget_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\t\trewards.append(reward.item())\n",
    "\t\tif done:\n",
    "\t\t\tprint(f\"{' ' * 50}\\repisode {i_episode} sum {sum(rewards)}\", end=\"\\r\")\n",
    "\t\t\tbreak\n",
    "\n",
    "# plt.plot(xvalues, temps, linewidth=0.1)\n",
    "# plt.plot(xvalues, target, linewidth=0.1)\n",
    "# plt.plot(xvalues, reward2, linewidth=0.1)\n",
    "# plt.ioff()\n",
    "# plt.savefig(\"out.png\", dpi=3000)\n",
    "\n",
    "torch.save(policy_net.state_dict(), \"subtract_4_3k_random_weather_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"subtract_4_3k_random_weather_2.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
